{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a061c707",
   "metadata": {},
   "source": [
    "#  DCGAN Improvement 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70d57b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os,cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from pylab import rcParams\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import mean\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from numpy.random import choice\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adam\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Concatenate\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.eager import def_function\n",
    "from tensorflow.python.keras import initializers\n",
    "from keras.initializers import RandomNormal\n",
    "from tensorflow.python.keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import plot_model\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from numpy import asarray\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4d9bca",
   "metadata": {},
   "source": [
    "## Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8e1ddf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12944, 48, 48, 1)\n",
      "(12944,)\n"
     ]
    }
   ],
   "source": [
    "#get the list of all files in the specified directory\n",
    "path = 'CK PLUS/CK+48'\n",
    "labels_list = os.listdir(path)\n",
    "labels_list\n",
    "\n",
    "images_data_list = []\n",
    "\n",
    "for dataset in labels_list:\n",
    "    imgs_per_label_list = os.listdir(path+'/'+ dataset)\n",
    "    for image in imgs_per_label_list:\n",
    "        #load the image in grayscale mode\n",
    "        input_image = cv2.imread(path + '/'+ dataset + '/'+ image,0)\n",
    "        #change the width and height of the original image to 48*48\n",
    "        image_resized = cv2.resize(input_image,(48,48))\n",
    "        #append the resized image to the images_data_list\n",
    "        images_data_list.append(image_resized)\n",
    "\n",
    "#convert data to numpy array\n",
    "images_data = np.array(images_data_list)\n",
    "images_data.shape\n",
    "\n",
    "number_of_classes = 7\n",
    "number_of_samples = images_data.shape[0]\n",
    "\n",
    "#create an 1D array of zeros with 981 elements.\n",
    "labels = np.zeros(number_of_samples,dtype='int64')\n",
    "\n",
    "labels[0:206]=0 #207\n",
    "labels[207:260]=1 #54\n",
    "labels[261:335]=2 #75\n",
    "labels[336:584]=3 #249\n",
    "labels[585:668]=4 #84\n",
    "labels[669:803]=5 #135\n",
    "labels[804:980]=6 #177\n",
    "\n",
    "#labels array\n",
    "y = np.array(labels)\n",
    "X = images_data.reshape(images_data.shape[0],48,48,1) \n",
    "\n",
    "#Shuffle the dataset\n",
    "X,y = shuffle(X,y, random_state=42)\n",
    "\n",
    "#Convert data into a float32 array \n",
    "X = X.astype('float32')\n",
    "# scale from [0,255] to [-1,1]\n",
    "X = (X - 127.5) / 127.5\n",
    "\n",
    "df = pd.read_csv('icml_face_data.csv')\n",
    "df_split = np.array_split(df, 3)\n",
    "df = df_split[0]\n",
    "\n",
    "df['emotion']=df['emotion'].replace({0:8,1:9,2:10,3:11,4:12,5:13,6:14}).replace({8:5,9:6,10:2,11:0,12:4,13:3,14:7})\n",
    "df['emotion'].unique()\n",
    "\n",
    "def preprocessing_data(df):  \n",
    "    img_array = np.zeros(shape=(len(df), 48, 48))\n",
    "    img_label = np.array(list(map(int, df['emotion']))) #map() function is used to iterate over an array\n",
    "    \n",
    "    for i, row in enumerate(df.index):\n",
    "        img = np.fromstring(df.loc[row,' pixels'], dtype=int, sep=' ')\n",
    "        img = np.reshape(img, (48, 48))\n",
    "        img_array[i] = img\n",
    "        \n",
    "    return img_array, img_label\n",
    "\n",
    "X_img, y_labels = preprocessing_data(df)\n",
    "\n",
    "#Reshape images into (48,48,1) shape\n",
    "X_img = X_img.reshape((X_img.shape[0], 48, 48, 1))\n",
    "#Convert data into a float32 array \n",
    "X_img = X_img.astype('float32')\n",
    "\n",
    "# scale from [0,255] to [-1,1]\n",
    "X_img = (X_img - 127.5) / 127.5\n",
    "\n",
    "X = np.concatenate((X, X_img))\n",
    "y = np.concatenate((y, y_labels))\n",
    "\n",
    "#Shuffle the dataset\n",
    "X,y = shuffle(X,y, random_state=42)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26b4ccc",
   "metadata": {},
   "source": [
    "### Mini batch Discrimination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24d5dfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinibatchDiscrimination(layers.Layer):\n",
    "\n",
    "    def __init__(self, num_kernel, dim_kernel,kernel_initializer='glorot_uniform', **kwargs):\n",
    "        self.num_kernel = num_kernel\n",
    "        self.dim_kernel = dim_kernel\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        super(MinibatchDiscrimination, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[1], self.num_kernel*self.dim_kernel),\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      trainable=True)\n",
    "        super(MinibatchDiscrimination, self).build(input_shape)  \n",
    "\n",
    "    def call(self, x):\n",
    "        activation = tf.matmul(x, self.kernel)\n",
    "        activation = tf.reshape(activation, shape=(-1, self.num_kernel, self.dim_kernel))\n",
    "        tmp1 = tf.expand_dims(activation, 3)\n",
    "        tmp2 = tf.transpose(activation, perm=[1, 2, 0])\n",
    "        tmp2 = tf.expand_dims(tmp2, 0)\n",
    "        \n",
    "        diff = tmp1 - tmp2\n",
    "        \n",
    "        l1 = tf.reduce_sum(tf.math.abs(diff), axis=2)\n",
    "        features = tf.reduce_sum(tf.math.exp(-l1), axis=2)\n",
    "        return tf.concat([x, features], axis=1)        \n",
    "        \n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1] + self.num_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d438f7",
   "metadata": {},
   "source": [
    "### Standalone Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3492e097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator():\n",
    "    # Gaussian Weight Initialization\n",
    "    init = RandomNormal(mean=0.0, stddev=0.02)\n",
    "    model = Sequential()\n",
    "    # downsample\n",
    "    model.add(Conv2D(128, 3, input_shape=(48,48,1)))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample\n",
    "    model.add(Conv2D(128,4,strides=2,padding='same',kernel_initializer=init))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample\n",
    "    model.add(Conv2D(128,4,strides=2,padding='same',kernel_initializer=init))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsample\n",
    "    model.add(Conv2D(64,4,strides=2,padding='same',kernel_initializer=init))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    #flatten\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    #Mini batch Discrimination\n",
    "    model.add(MinibatchDiscrimination(num_kernel=100, dim_kernel=5, name=\"mbd\"))\n",
    "    #classifier\n",
    "    model.add(Dense(64))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5, clipvalue=1.0)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6a4e426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 46, 46, 128)       1280      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 46, 46, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 23, 23, 128)       262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 23, 23, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 23, 23, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 12, 128)       262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 6, 6, 64)          131136    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 6, 6, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                147520    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "mbd (MinibatchDiscrimination (None, 164)               32000     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                10560     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 848,385\n",
      "Trainable params: 847,745\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-20 11:53:51.330573: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-20 11:53:51.331187: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# model summary\n",
    "discriminator_improv_3 = define_discriminator()\n",
    "discriminator_improv_3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5370ecc8",
   "metadata": {},
   "source": [
    "###  Keep all the rest and train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8a2178",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def define_generator():\n",
    "    # Gaussian Weight Initialization\n",
    "    init = RandomNormal(mean=0.0, stddev=0.02)\n",
    "    model = Sequential()\n",
    "    # Transforming the input into a 6*6 128-channel feature map \n",
    "    model.add(Dense(128*6*6,kernel_initializer=init,input_dim=100))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((6, 6, 128)))\n",
    "   # upsample to 12x12\n",
    "    model.add(Conv2DTranspose(128,4, strides=2, padding='same', kernel_initializer=init))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 24*24\n",
    "    model.add(Conv2DTranspose(128,4,strides=2, padding='same', kernel_initializer=init))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 48*48\n",
    "    model.add(Conv2DTranspose(64,4,strides=2, padding='same', kernel_initializer=init))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # Generator model, which maps the input of shape (dim_latent_space) into an image of shape (48,48,1)\n",
    "    model.add(Conv2D(1, 7, activation='tanh', padding='same', kernel_initializer=init))\n",
    "    return model\n",
    "\n",
    "def define_gan(generator, discriminator):\n",
    "    # Sets the discriminator weigths to non-trainable (this will only apply to the gan model)\n",
    "    discriminator.trainable = False\n",
    "    # connect them\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(generator)\n",
    "    # add the discriminator\n",
    "    model.add(discriminator)\n",
    "    # compile model\n",
    "    opt =  Adam(lr=0.0002, beta_1=0.5, clipvalue=1.0)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model\n",
    "\n",
    "# select real samples\n",
    "def real_samples(dataset,batch_size):\n",
    "    # choose random images\n",
    "    i = randint(0, dataset.shape[0],batch_size)\n",
    "    # select images\n",
    "    X = dataset[i]\n",
    "    # generate class labels\n",
    "    y = ones((batch_size, 1))\n",
    "    y = y*0.9\n",
    "    return X, y\n",
    "\n",
    "def fake_samples(generator, dim_latent_space, num_points):\n",
    "    # generate points in latent space\n",
    "    X_input = latent_points(dim_latent_space, num_points)\n",
    "    # predict outputs\n",
    "    X = generator.predict(X_input)\n",
    "    # create class labels\n",
    "    y = zeros((num_points, 1))\n",
    "    # number of labels to flip (5%)\n",
    "    num_to_flip = int(0.05 * y.shape[0])\n",
    "    # choose labels to flip\n",
    "    flip_xi = choice([i for i in range(y.shape[0])], size=num_to_flip)\n",
    "    # invert the labels in place\n",
    "    y[flip_xi] = 1 - y[flip_xi] \n",
    "    return X, y\n",
    "\n",
    "def latent_points(dim_latent_space,num_points):\n",
    "    # generate points in the latent space\n",
    "    X_input = randn(dim_latent_space * num_points) #Return samples from the “standard normal” distribution\n",
    "    # reshape into a batch of inputs for the network\n",
    "    X_input = X_input.reshape(num_points, dim_latent_space)\n",
    "    return X_input\n",
    "\n",
    "# create a line plot of loss for the gan and save to file\n",
    "def plot_history(loss1_disc,loss2_disc, g_loss_hist):\n",
    "    # plot history\n",
    "    plt.plot(loss1_disc, label='discriminator_real_loss')\n",
    "    plt.plot(loss2_disc, label='discriminator_fake_loss')\n",
    "    plt.plot(g_loss_hist, label='generator_loss')\n",
    "    plt.legend()\n",
    "    plt.close()\n",
    "\n",
    "def train(generator_model, discriminator_model, gan_model, dataset, dim_latent_space, num_epochs=9, batch_size=128):\n",
    "    batch_per_epoch = int(dataset.shape[0] / batch_size)\n",
    "    half_batch = int(batch_size / 2)\n",
    "    # lists for keeping track of loss\n",
    "    loss1_disc,loss2_disc, g_loss_hist = list(), list(), list()\n",
    "    # manually enumerate epochs\n",
    "    for i in range(num_epochs):\n",
    "        loss1, loss2, g_loss = list(), list(),list()\n",
    "        # enumerate batches over the training set\n",
    "        for j in range(batch_per_epoch):\n",
    "            # get randomly selected \"real\" samples\n",
    "            X_real, y_real = real_samples(dataset, half_batch)\n",
    "            # update discriminator model weights, One sided label smoothing\n",
    "            discriminator_loss1, _ = discriminator_model.train_on_batch(X_real, y_real) #train_on_batch allows you to expressly update weights based on a collection of samples you provide, without regard to any fixed batch size.\n",
    "            loss1.append(discriminator_loss1)\n",
    "            # generate \"fake\" samples\n",
    "            X_fake, y_fake = fake_samples(generator_model, dim_latent_space, half_batch)\n",
    "            # update discriminator model weights\n",
    "            discriminator_loss2, _ = discriminator_model.train_on_batch(X_fake, y_fake)\n",
    "            loss2.append(discriminator_loss2)\n",
    "            # Produce points in latent space to use as the generator's input\n",
    "            X_gan = latent_points(dim_latent_space,batch_size)\n",
    "            # create inverted labels for the fake samples\n",
    "            y_gan = ones((batch_size, 1))\n",
    "            # update the generator via the discriminator's error\n",
    "            generator_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "            g_loss.append(generator_loss)\n",
    "            # summarize loss on this batch\n",
    "            print('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' %\n",
    "                (i+1, j+1, batch_per_epoch, discriminator_loss1, discriminator_loss2, generator_loss))\n",
    "        # store losses per epoch \n",
    "        loss1_disc.append(mean(loss1))\n",
    "        loss2_disc.append(mean(loss2))\n",
    "        g_loss_hist.append(mean(g_loss))\n",
    "    # save generator model to disk\n",
    "    filename = 'generator_improv_3.sav'\n",
    "    pickle.dump(generator_model, open(filename, 'wb'))\n",
    "    # line plots of loss\n",
    "    plot_history(loss1_disc,loss2_disc, g_loss_hist)\n",
    "\n",
    "# size of the latent space\n",
    "dim_latent_space = 100\n",
    "# create the discriminator\n",
    "discriminator_model= define_discriminator()\n",
    "# create the generator\n",
    "generator_model = define_generator()\n",
    "# create the gan\n",
    "gan_model = define_gan(generator_model, discriminator_model)\n",
    "# load image data\n",
    "dataset = X\n",
    "# train model\n",
    "train(generator_model,discriminator_model, gan_model, dataset, dim_latent_space)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfc2881",
   "metadata": {},
   "source": [
    "### Loading the generator model and generating images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9952d3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and save a plot of generated images (reversed grayscale)\n",
    "def g_img_plot(samples, n):\n",
    "    # plot images\n",
    "    for i in range(n * n):\n",
    "        # define subplot\n",
    "        plt.subplot(n, n, 1 + i)\n",
    "        # turn off axis\n",
    "        plt.axis('off')\n",
    "        # plot raw pixel data\n",
    "        plt.imshow(samples[i, :, :, 0], cmap='gray_r')\n",
    "    plt.show()\n",
    "\n",
    "# load the model from disk\n",
    "model = pickle.load(open('generator_improv_3.sav', 'rb'))\n",
    "# generate images\n",
    "latent_points = latent_points(100, 100)\n",
    "# generate images\n",
    "output = model.predict(latent_points)\n",
    "# plot generated images\n",
    "g_img_plot(output,4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
